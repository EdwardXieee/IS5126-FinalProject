{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection and Key Purpose\n",
    "Our key dataset is Divvy Bike Share Service data, from https://divvy-tripdata.s3.amazonaws.com/index.html. We selected data in year 2019, which consists of 4 csv files,from Q1 to Q4.\n",
    "\n",
    "(Key Purpose to be filled up after machine learning part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file paths for each quarter's data\n",
    "file_paths = {\n",
    "    'Q1': 'data/Divvy_Trips_2019_Q1.csv',\n",
    "    'Q2': 'data/Divvy_Trips_2019_Q2.csv',\n",
    "    'Q3': 'data/Divvy_Trips_2019_Q3.csv',\n",
    "    'Q4': 'data/Divvy_Trips_2019_Q4.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the 4 files share identical column names in order to concatenate all quarters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column names in Q1:\n",
      "['trip_id', 'start_time', 'end_time', 'bikeid', 'tripduration', 'from_station_id', 'from_station_name', 'to_station_id', 'to_station_name', 'usertype', 'gender', 'birthyear']\n",
      "\n",
      "Column names in Q2:\n",
      "['01 - Rental Details Rental ID', '01 - Rental Details Local Start Time', '01 - Rental Details Local End Time', '01 - Rental Details Bike ID', '01 - Rental Details Duration In Seconds Uncapped', '03 - Rental Start Station ID', '03 - Rental Start Station Name', '02 - Rental End Station ID', '02 - Rental End Station Name', 'User Type', 'Member Gender', '05 - Member Details Member Birthday Year']\n",
      "\n",
      "Column names in Q3:\n",
      "['trip_id', 'start_time', 'end_time', 'bikeid', 'tripduration', 'from_station_id', 'from_station_name', 'to_station_id', 'to_station_name', 'usertype', 'gender', 'birthyear']\n",
      "\n",
      "Column names in Q4:\n",
      "['trip_id', 'start_time', 'end_time', 'bikeid', 'tripduration', 'from_station_id', 'from_station_name', 'to_station_id', 'to_station_name', 'usertype', 'gender', 'birthyear']\n"
     ]
    }
   ],
   "source": [
    "# Display the column names of each file to see the differences\n",
    "for key, file_path in file_paths.items():\n",
    "    df = pd.read_csv(file_path, nrows=1)  # Load just the first row for speed\n",
    "    print(f\"\\nColumn names in {key}:\")\n",
    "    print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that from the previous output, Q2 has different column names. Thus, we need to convert them to the same naming convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column names in Q1 after renaming:\n",
      "['trip_id', 'start_time', 'end_time', 'bikeid', 'tripduration', 'from_station_id', 'from_station_name', 'to_station_id', 'to_station_name', 'usertype', 'gender', 'birthyear']\n",
      "\n",
      "Column names in Q2 after renaming:\n",
      "['trip_id', 'start_time', 'end_time', 'bikeid', 'tripduration', 'from_station_id', 'from_station_name', 'to_station_id', 'to_station_name', 'usertype', 'gender', 'birthyear']\n",
      "\n",
      "Column names in Q3 after renaming:\n",
      "['trip_id', 'start_time', 'end_time', 'bikeid', 'tripduration', 'from_station_id', 'from_station_name', 'to_station_id', 'to_station_name', 'usertype', 'gender', 'birthyear']\n",
      "\n",
      "Column names in Q4 after renaming:\n",
      "['trip_id', 'start_time', 'end_time', 'bikeid', 'tripduration', 'from_station_id', 'from_station_name', 'to_station_id', 'to_station_name', 'usertype', 'gender', 'birthyear']\n"
     ]
    }
   ],
   "source": [
    "# Define the correct column names for consistency\n",
    "correct_columns = [\n",
    "    'trip_id', 'start_time', 'end_time', 'bikeid', 'tripduration', \n",
    "    'from_station_id', 'from_station_name', 'to_station_id', \n",
    "    'to_station_name', 'usertype', 'gender', 'birthyear'\n",
    "]\n",
    "\n",
    "# Load each CSV file, rename columns, and store in a list\n",
    "dfs = []\n",
    "for key, file_path in file_paths.items():\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Rename columns for Q2 as they do not match\n",
    "    if key == 'Q2':\n",
    "        df.columns = correct_columns\n",
    "    dfs.append(df)\n",
    "\n",
    "    # Display column names after renaming for verification\n",
    "    print(f\"\\nColumn names in {key} after renaming:\")\n",
    "    print(df.columns.tolist())\n",
    "\n",
    "# Concatenate all quarters into a single DataFrame\n",
    "df_all_quarters = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**\n",
    "\n",
    "Convert the format of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'start_time' and 'end_time' to datetime for easier manipulation\n",
    "df_all_quarters['start_time'] = pd.to_datetime(df_all_quarters['start_time'], errors='coerce')\n",
    "df_all_quarters['end_time'] = pd.to_datetime(df_all_quarters['end_time'], errors='coerce')\n",
    "\n",
    "# Convert 'tripduration' to a numeric value (remove any commas and convert to float)\n",
    "df_all_quarters['tripduration'] = pd.to_numeric(df_all_quarters['tripduration'].str.replace(',', ''), errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**\n",
    "\n",
    "Check missing values and replace them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'start_time': 0\n",
      "Missing values in 'end_time': 0\n",
      "Missing values in 'tripduration': 0\n",
      "Missing values in 'usertype': 0\n",
      "Missing values in 'gender': 559206\n",
      "Missing values in 'birthyear': 538751\n"
     ]
    }
   ],
   "source": [
    "# Find the number of missing values in 'gender' and 'birthyear'\n",
    "print(\"Missing values in 'start_time':\", df_all_quarters['start_time'].isnull().sum())\n",
    "print(\"Missing values in 'end_time':\", df_all_quarters['end_time'].isnull().sum())\n",
    "print(\"Missing values in 'tripduration':\", df_all_quarters['tripduration'].isnull().sum())\n",
    "print(\"Missing values in 'usertype':\", df_all_quarters['usertype'].isnull().sum())\n",
    "print(\"Missing values in 'gender':\", df_all_quarters['gender'].isnull().sum())\n",
    "print(\"Missing values in 'birthyear':\", df_all_quarters['birthyear'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in 'gender' after replacement: 0\n",
      "\n",
      "Missing values in 'birthyear' after replacement: 0\n"
     ]
    }
   ],
   "source": [
    "# Replace missing values in 'gender' with \"Unknown\"\n",
    "df_all_quarters['gender'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Convert 'birthyear' to integers, filling missing values with a placeholder (e.g., 0 for unknown)\n",
    "df_all_quarters['birthyear'] = df_all_quarters['birthyear'].fillna(0).astype(int)\n",
    "\n",
    "# Check missing values in 'gender' after replacement\n",
    "print(\"\\nMissing values in 'gender' after replacement:\", df_all_quarters['gender'].isnull().sum())\n",
    "\n",
    "# Check missing values in 'birthyear' after replacement\n",
    "print(\"\\nMissing values in 'birthyear' after replacement:\", df_all_quarters['birthyear'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**\n",
    "\n",
    "Check Duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of duplicate rows before removal: 0\n",
      "Number of duplicate rows after removal: 0\n"
     ]
    }
   ],
   "source": [
    "# Check the number of duplicates\n",
    "print(\"\\nNumber of duplicate rows before removal:\", df_all_quarters.duplicated().sum())\n",
    "\n",
    "# Remove duplicates\n",
    "df_all_quarters.drop_duplicates(inplace=True)\n",
    "\n",
    "# Check the number of duplicates after removal\n",
    "print(\"Number of duplicate rows after removal:\", df_all_quarters.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**\n",
    "\n",
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column to categorize 'birthyear' into age groups\n",
    "def categorize_birthyear(birthyear):\n",
    "    if birthyear == 0:\n",
    "        return \"Unknown\"\n",
    "    elif birthyear < 1960:\n",
    "        return \"before 1960\"\n",
    "    elif 1960 <= birthyear < 1970:\n",
    "        return \"1960-1969\"\n",
    "    elif 1970 <= birthyear < 1980:\n",
    "        return \"1970-1979\"\n",
    "    elif 1980 <= birthyear < 1990:\n",
    "        return \"1980-1989\"\n",
    "    elif 1990 <= birthyear < 2000:\n",
    "        return \"1990-1999\"\n",
    "    else:\n",
    "        return \"after 1999\"\n",
    "\n",
    "df_all_quarters['age_group'] = df_all_quarters['birthyear'].apply(categorize_birthyear)\n",
    "\n",
    "# Add columns for 'start_month' and 'start_day_of_week'\n",
    "df_all_quarters['start_month'] = df_all_quarters['start_time'].dt.strftime('%b')  # Jan, Feb, ..., Dec\n",
    "df_all_quarters['start_day_of_week'] = df_all_quarters['start_time'].dt.strftime('%a')  # Mon, Tue, ..., Sun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of processed data:\n",
      "   gender  birthyear  age_group start_month start_day_of_week\n",
      "0    Male       1989  1980-1989         Jan               Tue\n",
      "1  Female       1990  1990-1999         Jan               Tue\n",
      "2  Female       1994  1990-1999         Jan               Tue\n",
      "3    Male       1993  1990-1999         Jan               Tue\n",
      "4    Male       1994  1990-1999         Jan               Tue\n"
     ]
    }
   ],
   "source": [
    "# Display a few rows to verify the changes\n",
    "print(\"\\nSample of processed data:\")\n",
    "print(df_all_quarters[['gender', 'birthyear', 'age_group', 'start_month', 'start_day_of_week']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
